{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6a923c-7b18-4b83-8f1e-2cfb74532c15",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It provides valuable insights for prediction and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34667f0f-7953-48d0-be0f-235eac2cc858",
   "metadata": {},
   "source": [
    "## Importance\n",
    "The interpretability of linear regression is one of its greatest strengths. The model's equation offers clear coefficients that illustrate the influence of each independent variable on the dependent variable, enhancing our understanding of the underlying relationships. Its simplicity is a significant advantage; linear regression is transparent, easy to implement, and serves as a foundational concept for more advanced algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d5f78-89b9-4020-8e26-c05bdadc0978",
   "metadata": {},
   "source": [
    "## Best Fit Line\n",
    "Our primary objective while using linear regression is to locate the best-fit line, which implies that the error between the predicted and actual values should be kept to a minimum. There will be the least error in the best-fit line.\n",
    "\n",
    "The best Fit Line equation provides a straight line that represents the relationship between the dependent and independent variables. The slope of the line indicates how much the dependent variable changes for a unit change in the independent variable(s).\n",
    "\n",
    "![Best Fit Line](https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png \"Best Fit Line\")\n",
    "\n",
    "Here Y is called a dependent or target variable and X is called an independent variable also known as the predictor of Y. There are many types of functions or modules that can be used for regression. A linear function is the simplest type of function. Here, X may be a single feature or multiple features representing the problem.\n",
    "\n",
    "Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x)). Hence, the name is Linear Regression. In the figure above, X (input) is the work experience and Y (output) is the salary of a person. The regression line is the best-fit line for our model.\n",
    "\n",
    "In linear regression some hypothesis are made to ensure reliability of the model's results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008af62-57ca-4fd6-92f9-532c513d5b20",
   "metadata": {},
   "source": [
    "## Assumptions \n",
    "\n",
    "- **Linearity**: It assumes that there is a linear relationship between the independent and dependent variables. This means that changes in the independent variable lead to proportional changes in the dependent variable.\n",
    "- **Independence**: The observations should be independent from each other that is the errors from one observation should not influence other.\n",
    "\n",
    "Let's assume there is a linear relationship between X and Y then\n",
    "\n",
    "$$\\hat{Y} = \\theta_1 + \\theta_2 X$$\n",
    "$$or$$\n",
    "$$\\hat{y_i} = \\theta_1 + \\theta_2 x_i$$\n",
    "\n",
    "$$y_i \\in Y = \\text{are labels to data (Supervised learning)}$$\n",
    "$$x_i \\in X = \\text{are the input independent training data (univariate - one input variable(parameter)) }$$\n",
    "$$\\hat{y_i} \\in Y = \\text{are the predicted values.}$$\n",
    "\n",
    "$$\\text{The model gets the best regression fit line by finding the best $\\theta_1$ and $\\theta_2$ values.}$$\n",
    "\n",
    "$$\\theta_1 =  intercept$$ \n",
    "$$\\theta_2 =  \\text{coefficient of x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85e1c0-5cec-409e-933e-5a40afe60b39",
   "metadata": {},
   "source": [
    "## Update intercept values to get the best-fit line\n",
    "To achieve the best-fit regression line, the model aims to predict the target value $\\hat{Y}$ such that the error difference between the predicted value $\\hat{Y}$ and the true value Y is minimum. So, it is very important to update the $\\theta_1$ and $\\theta_2$ values, to reach the best value that minimizes the error between the predicted y value (pred) and the true y value (y). \n",
    "\n",
    "$$minimize \\frac{1}{n} \\sum_1^n(\\bar{y_1} - y)^2$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d41d1b-1314-4441-8cfa-925fc2677da5",
   "metadata": {},
   "source": [
    "## Types of Linear Regression\n",
    "When there is only one independent feature it is known as Simple Linear Regression or Univariate Linear Regression and when there are more than one feature it is known as Multiple Linear Regression or Multivariate Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80853c34-6117-4d6f-87a2-248f7fce2050",
   "metadata": {},
   "source": [
    "### Simple Linear Regression\n",
    "Simple linear regression is the simplest form of linear regression and it involves only one independent variable and one dependent variable. The equation for simple linear regression is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1X_1 + \\epsilon$$\n",
    "$$Y  = \\text{is the dependent variable}$$\n",
    "$$X  = \\text{is the independent variable}$$\n",
    "$$\\beta_0 = \\text{is the intercept}$$\n",
    "$$\\beta_1 = \\text{is the slope}$$\n",
    "$$\\epsilon = \\text{error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c0388-3eb3-4817-a8d4-6b10cc592f65",
   "metadata": {},
   "source": [
    "### Assumptions of Simple Linear Regression\n",
    "- **Linearity**: The independent and dependent variables have a linear relationship with one another. This implies that changes in the dependent variable follow those in the independent variable(s) in a linear fashion. This means that there should be a straight line that can be drawn through the data points. If the relationship is not linear, then linear regression will not be an accurate model.\n",
    "\n",
    "![Linearity](https://media.geeksforgeeks.org/wp-content/uploads/20231123113044/python-linear-regression-4.png \"Linearity\")\n",
    "\n",
    "- **Independence**: The observations in the dataset are independent of each other. This means that the value of the dependent variable for one observation does not depend on the value of the dependent variable for another observation. If the observations are not independent, then linear regression will not be an accurate model.\n",
    "- **Homoscedasticity**: Across all levels of the independent variable(s), the variance of the errors is constant. This indicates that the amount of the independent variable(s) has no impact on the variance of the errors. If the variance of the residuals is not constant, then linear regression will not be an accurate model.\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20231123113103/python-linear-regression-5.png)\n",
    "\n",
    "- **Normality**: The residuals should be normally distributed. This means that the residuals should follow a bell-shaped curve. If the residuals are not normally distributed, then linear regression will not be an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094540e-409d-474c-9055-536ea6cf89ce",
   "metadata": {},
   "source": [
    "### Use Case of Simple Linear Regression\n",
    "- In a case study evaluating student performance analysts use simple linear regression to examine the relationship between study hours and exam scores. By collecting data on the number of hours students studied and their corresponding exam results the analysts developed a model that reveal correlation, for each additional hour spent studying, students exam scores increased by an average of 5 points. This case highlights the utility of simple linear regression in understanding and improving academic performance.\n",
    "\n",
    "- Another case study focus on marketing and sales where businesses uses simple linear regression to forecast sales based on historical data particularly examining how factors like advertising expenditure influence revenue. By collecting data on past advertising spending and corresponding sales figures analysts develop a regression model that tells the relationship between these variables. For instance if the analysis reveals that for every additional dollar spent on advertising sales increase by $10. This predictive capability enables companies to optimize their advertising strategies and allocate resources effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc121f-0069-4420-b9cc-774744e57355",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Multiple Linear Regression\n",
    "Multiple linear regression involves more than one independent variable and one dependent variable. The equation for multiple linear regression is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ..... + \\beta_nX_n$$\n",
    "\n",
    "$$Y = \\text{is the dependent variable}$$\n",
    "$$X1, X2, ..., Xn = \\text{are the independent variables}$$\n",
    "$$\\beta_0 = \\text{is the intercept}$$\n",
    "$$\\beta_1, \\beta_2, ..., \\beta_n = \\text{are the slopes}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e492aa-1b7e-4117-9bfd-ccd8635591a2",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "- **No multicollinearity**: There is no high correlation between the independent variables. This indicates that there is little or no correlation between the independent variables. Multicollinearity occurs when two or more independent variables are highly correlated with each other, which can make it difficult to determine the individual effect of each variable on the dependent variable. If there is multicollinearity, then multiple linear regression will not be an accurate model.\n",
    "- **Additivity**: The model assumes that the effect of changes in a predictor variable on the response variable is consistent regardless of the values of the other variables. This assumption implies that there is no interaction between variables in their effects on the dependent variable.\n",
    "- **Feature Selection**: In multiple linear regression, it is essential to carefully select the independent variables that will be included in the model. Including irrelevant or redundant variables may lead to overfitting and complicate the interpretation of the model.\n",
    "- **Overfitting**: Overfitting occurs when the model fits the training data too closely, capturing noise or random fluctuations that do not represent the true underlying relationship between variables. This can lead to poor generalization performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fb9c2-9ac1-49f4-adf0-ba4ba9e98ff2",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "Multicollinearity is a statistical phenomenon where two or more independent variables in a multiple regression model are highly correlated, making it difficult to assess the individual effects of each variable on the dependent variable.\n",
    "\n",
    "Detecting Multicollinearity includes two techniques:\n",
    "\n",
    "- **Correlation Matrix**: Examining the correlation matrix among the independent variables is a common way to detect multicollinearity. High correlations (close to 1 or -1) indicate potential multicollinearity.\n",
    "- **VIF (Variance Inflation Factor)**: VIF is a measure that quantifies how much the variance of an estimated regression coefficient increases if your predictors are correlated. A high VIF (typically above 10) suggests multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcb800f-31d1-4c57-a005-4c78e044c322",
   "metadata": {},
   "source": [
    "## Use Case of Multiple Linear Regression\n",
    "Multiple linear regression allows us to analyze relationship between multiple independent variables and a single dependent variable. Here are some use cases:\n",
    "\n",
    "- **Real Estate Pricing**: In real estate MLR is used to predict property prices based on multiple factors such as location, size, number of bedrooms, etc. This helps buyers and sellers understand market trends and set competitive prices.\n",
    "- **Financial Forecasting**: Financial analysts use MLR to predict stock prices or economic indicators based on multiple influencing factors such as interest rates, inflation rates and market trends. This enables better investment strategies and risk management24.\n",
    "- **Agricultural Yield Prediction**: Farmers can use MLR to estimate crop yields based on several variables like rainfall, temperature, soil quality and fertilizer usage. This information helps in planning agricultural practices for optimal productivity\n",
    "- **E-commerce Sales Analysis**: An e-commerce company can utilize MLR to assess how various factors such as product price, marketing promotions and seasonal trends impact sales.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddeaa48-dd15-4d95-98a0-52255b89d9a7",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Linear Regression\n",
    "1. **Mean Square Error (MSE)**\n",
    "Mean Squared Error (MSE) is an evaluation metric that calculates the average of the squared differences between the actual and predicted values for all the data points. The difference is squared to ensure that negative and positive differences don't cancel each other out.\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_1^n(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "$$n = \\text{is the number of data points.}$$\n",
    "$$y_i = \\text{is the actual or observed value for the ith data point.}$$\n",
    "$$\\hat{y_i} = \\text{is the predicted value for the ith data point.}$$\n",
    "\n",
    "**MSE is a way to quantify the accuracy of a model's predictions. MSE is sensitive to outliers as large errors contribute significantly to the overall score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d43d5-f837-4ea9-8560-81756d92241b",
   "metadata": {},
   "source": [
    "## Mean Absolute Error (MAE)\n",
    "Mean Absolute Error is an evaluation metric used to calculate the accuracy of a regression model. MAE measures the average absolute difference between the predicted values and actual values.\n",
    "\n",
    "$$ MAE = \\frac{1}{n} \\sum_1^n(|Y_i - \\hat{Y}|)$$\n",
    "\n",
    "$$n = \\text{is the number of data points.}$$\n",
    "$$Y_i = \\text{is the actual or observed value for the ith data point.}$$\n",
    "$$\\hat{Y_i} = \\text{is the predicted value for the ith data point.}$$\n",
    "\n",
    "**Lower MAE value indicates better model performance. It is not sensitive to the outliers as we consider absolute differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e8f68-df82-4600-bf5f-91992fa73b19",
   "metadata": {},
   "source": [
    "## Root Mean Squared Error (RMSE)\n",
    "The square root of the residuals' variance is the Root Mean Squared Error. It describes how well the observed data points match the expected values, or the model's absolute fit to the data.\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{RSS}{n}}$$\n",
    "$$RSS = \\sum_2^n(y_i^\\text{actual} - y_i^\\text{predicted})^2$$\n",
    "\n",
    "**RSME is not as good of a metric as R-squared. Root Mean Squared Error can fluctuate when the units of the variables vary since its value is dependent on the variables' units (it is not a normalized measure).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd609893-8052-4725-b7ae-02cbacf88c0d",
   "metadata": {},
   "source": [
    "## Coefficient of Determination (R-squared)\n",
    "R-Squared is a statistic that indicates how much variation the developed model can explain or capture. It is always in the range of 0 to 1. In general, the better the model matches the data, the greater the R-squared number.\n",
    "In mathematical notation, it can be expressed as:\n",
    "$$R^2 = 1 - (\\frac{RSS}{TSS})$$\n",
    "\n",
    "- **Residual sum of Squares (RSS)**: The sum of squares of the residual for each data point in the plot or data is known as the residual sum of squares, or RSS. It is a measurement of the difference between the output that was observed and what was anticipated.\n",
    "\n",
    "$$RSS = \\sum_2^n(y_i - b_0 - b_1x_i)^2$$\n",
    "\n",
    "- **Total Sum of Squares (TSS)**: The sum of the data points' errors from the answer variable's mean is known as the total sum of squares, or TSS.\n",
    "$$TSS = \\sum(y - \\bar{y_i})^2$$\n",
    "\n",
    "**R squared metric is a measure of the proportion of variance in the dependent variable that is explained the independent variables in the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d2cd6-b58e-44e0-a354-66c71c99702c",
   "metadata": {},
   "source": [
    "## Degrees of Freedom\n",
    "The degrees of freedom (DoF) in linear regression generally refer to the number of independent data points available for estimating statistical parameters.\n",
    "\n",
    "$$DOF = n - (k + 1)$$\n",
    "$$n = \\text{is the number of observations}$$\n",
    "$$k = \\text{is the number of predictors in the model}$$\n",
    "\n",
    "\n",
    "The more variables to account for the less the degrees of freedom becomes\n",
    "\n",
    "Even though adding extra variables increase the r^2 value but adding variables which are unnecessary decreases the adjusted r^2 value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861e5f5-6b59-4da9-81ca-5dfcad418b7e",
   "metadata": {},
   "source": [
    "## Adjusted R-Squared Error\n",
    "Adjusted $R^2$ measures the proportion of variance in the dependent variable that is explained by independent variables in a regression model. Adjusted R-square accounts the number of predictors in the model and penalizes the model for including irrelevant predictors that don't contribute significantly to explain the variance in the dependent variables.\n",
    "\n",
    "Mathematically, adjusted $R^2$ is expressed as:\n",
    "\n",
    "$$Adjusted R^2 = 1 - (\\frac{(1 - R^2)(n - 1)}{n - k - 1})$$\n",
    "\n",
    "$$n = \\text{is the number of observations}$$\n",
    "$$k = \\text{is the number of predictors in the model}$$\n",
    "$$R^2 = \\text{is coeeficient of determination}$$\n",
    "\n",
    "**Adjusted R-square helps to prevent overfitting. It penalizes the model with additional predictors that do not contribute significantly to explain the variance in the dependent variable.**\n",
    "\n",
    "**While evaluation metrics help us measure the performance of a model, regularization helps in improving that performance by addressing overfitting and enhancing generalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd5a2e-9faa-4906-87a4-2aba5226ea4e",
   "metadata": {},
   "source": [
    "## +/- ve regression line\n",
    "- **Positive Linear Regression Line**: A positive linear regression line indicates a direct relationship between the independent variable (X) and the dependent variable (Y). This means that as the value of X increases, the value of Y also increases. The slope of a positive linear regression line is positive, meaning that the line slants upward from left to right.\n",
    "- **Negative Linear Regression Line**: A negative linear regression line indicates an inverse relationship between the independent variable (X) and the dependent variable (Y). This means that as the value of X increases, the value of Y decreases. The slope of a negative linear regression line is negative, meaning that the line slants downward from left to right.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa448d-f0ab-48ce-8b5a-4610342b90d8",
   "metadata": {},
   "source": [
    "## Applications of Linear Regression\n",
    "Linear regression is used in many different fields including finance, economics and psychology to understand and predict the behavior of a particular variable.\n",
    "\n",
    "For example linear regression is widely used in finance to analyze relationships and make predictions. It can model how a company's earnings per share (EPS) influence its stock price. If the model shows that a 1 dollar increase in EPS results in a $15 rise in stock price, investors gain insights into the company's valuation. Similarly, linear regression can forecast currency values by analyzing historical exchange rates and economic indicators, helping financial professionals make informed decisions and manage risks effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36422c67-6e86-473a-9b72-6005a43a0e90",
   "metadata": {},
   "source": [
    "## Advantages of Linear Regression\n",
    "- Linear regression is a relatively simple algorithm, making it easy to understand and implement. The coefficients of the linear regression model can be interpreted as the change in the dependent variable for a one-unit change in the independent variable, providing insights into the relationships between variables.\n",
    "- Linear regression is computationally efficient and can handle large datasets effectively. It can be trained quickly on large datasets, making it suitable for real-time applications.\n",
    "- Linear regression is relatively robust to outliers compared to other machine learning algorithms. Outliers may have a smaller impact on the overall model performance.\n",
    "- Linear regression often serves as a good baseline model for comparison with more complex machine learning algorithms.\n",
    "- Linear regression is a well-established algorithm with a rich history and is widely available in various machine learning libraries and software packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf0baf-df95-40b1-a6dd-d4b0f6f22a7c",
   "metadata": {},
   "source": [
    "## Disadvantages of Linear Regression\n",
    "- Linear regression assumes a linear relationship between the dependent and independent variables. If the relationship is not linear, the model may not perform well.\n",
    "- Linear regression is sensitive to multicollinearity, which occurs when there is a high correlation between independent variables. Multicollinearity can inflate the variance of the coefficients and lead to unstable model predictions.\n",
    "- Linear regression assumes that the features are already in a suitable form for the model. Feature engineering may be required to transform features into a format that can be effectively used by the model.\n",
    "- Linear regression is susceptible to both overfitting and underfitting. Overfitting occurs when the model learns the training data too well and fails to generalize to unseen data. Underfitting occurs when the model is too simple to capture the underlying relationships in the data.\n",
    "- Linear regression provides limited explanatory power for complex relationships between variables. More advanced machine learning techniques may be necessary for deeper insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1456e5a-fd66-4670-8d07-3382608c7b90",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe6cf8-0185-481f-ab65-d17ac1e5c8e9",
   "metadata": {},
   "source": [
    "## Definition\n",
    "Polynomial Regression is a form of regression analysis where the relationship between the independent variable `x` and the dependent variable `y` is modeled as an `n`th-degree polynomial. It is a special case of **Multiple Linear Regression** where the predictors are powers of a single variable.\n",
    "\n",
    "The model looks like:\n",
    "$$y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\cdots + \\beta_nx^n + \\epsilon$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391fd6e-3eb8-4f3a-8ee9-7e7a69e260e1",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "1. **Feature Transformation**: Polynomial regression transforms the original input features into higher-degree features (e.g., $x^2, x^3, \\ldots, x^n$).\n",
    "2. **Linear Model Fitting**: A linear model is fit on these transformed features using least squares or other optimization methods.\n",
    "3. **Prediction**: The fitted polynomial function is used to predict `y` from `x`.\n",
    "\n",
    "> Note: Although the relationship is nonlinear, the model is still linear in terms of coefficients (β), hence it's considered a linear model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e11ae7-4c55-4ba0-b403-2abd4c036c6c",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "- **Underfitting**:  \n",
    "  Happens when the polynomial degree is too low to capture the underlying trend (e.g., fitting a line to a curve).\n",
    "\n",
    "- **Overfitting**:  \n",
    "  Happens when the degree is too high, and the model captures noise rather than the true pattern, performing poorly on unseen data.\n",
    "\n",
    "> A good model balances both by choosing an optimal degree via techniques like cross-validation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0634817-519a-490d-846f-79f243b917ef",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "- Can model non-linear relationships.\n",
    "- More flexible than linear regression.\n",
    "- Easy to implement and interpret for lower degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524f307-9dd2-42d2-a882-788be5aa3416",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "- Sensitive to outliers.\n",
    "- High-degree polynomials may overfit.\n",
    "- Interpretation becomes complex with higher degrees.\n",
    "- May lead to unstable predictions outside the range of training data (extrapolation issue).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10669f6b-bccc-47b2-8588-65629bc8409c",
   "metadata": {},
   "source": [
    "## Difference: Linear Regression vs Polynomial Regression\n",
    "\n",
    "| Aspect                    | Linear Regression             | Polynomial Regression                          |\n",
    "|--------------------------|-------------------------------|------------------------------------------------|\n",
    "| Relationship Modeled     | Linear                        | Non-linear (polynomial)                        |\n",
    "| Equation Form            | $y = \\beta_0 + \\beta_1x$    | $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\ldots$ |\n",
    "| Flexibility              | Low                           | High (can model curves)                        |\n",
    "| Risk of Overfitting      | Low                           | Higher (with high degrees)                     |\n",
    "| Feature Engineering      | None                          | Requires adding polynomial terms               |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
